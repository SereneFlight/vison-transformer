# ViT 从零实现 - 学习指南

> 🎯 **目标**: 完全理解并手写 Vision Transformer
>
> 📅 **开始日期**: 2025-12-07
>
> 👤 **学习者**: 保研到灵巧手方向，VLA 研究

---

## ✅ 已完成

### 1. 项目准备
- [x] 创建项目文件夹 `~/桌面/vla_learning/vit_from_scratch/`
- [x] 编写完整代码（第一遍示范）
- [x] 测试所有模块（全部通过 ✅）

### 2. 文档准备
- [x] **README.md** - 项目大纲和整体架构
- [x] **vit_architecture.md** - 详细数学推导和维度变化
- [x] **vit_model.py** - 完整实现（带超详细注释）
- [x] **test_model.py** - 测试每个模块

---

## 📂 项目文件说明

```
vit_from_scratch/
├── README.md              ← 从这里开始！整体架构流程图
├── vit_architecture.md    ← 数学原理详解（非常详细）
├── vit_model.py           ← 完整代码（500+ 行，带注释）
├── test_model.py          ← 测试代码（理解维度变化）
└── 学习指南.md            ← 本文件
```

---

## 🎓 学习路线（建议）

### 阶段 1: 理解架构（今天）

**时间**: 2-3 小时

**步骤**:
1. **先看 README.md**
   - 理解整体流程图（从输入图像到分类结果）
   - 看懂 5 个核心模块的作用
   - 记住关键参数（768 维度、12 层、12 头等）

2. **再看 vit_architecture.md**
   - **重点看第 3 节**: Multi-Head Attention（最难的部分）
   - 理解 Q、K、V 的含义
   - 理解为什么要除以 √d
   - 理解多头的意义

3. **运行测试代码**
   ```bash
   cd ~/桌面/vla_learning/vit_from_scratch
   python3 test_model.py
   ```
   - 看每一层的输出形状
   - 理解维度是怎么变化的

4. **阅读 vit_model.py**
   - **不要着急敲代码！**
   - 先通读一遍，看注释
   - 重点理解 `Attention.forward()` 的每一步

**检查点**:
- [ ] 能画出 ViT 的整体流程图
- [ ] 能解释什么是 Patch Embedding
- [ ] 能解释 Attention 的计算流程（Q、K、V）
- [ ] 知道为什么要有 CLS token

---

### 阶段 2: 手写代码（明天或后天）

**时间**: 4-6 小时

**注意**:
- **第一遍不要求完全自己写！**
- 可以对着我的代码，一边理解一边敲
- 重点是**理解每一行的作用**

**建议顺序**:

#### Step 1: PatchEmbed (最简单)
```python
# 只需要理解 3 个操作:
1. Conv2d(3, 768, kernel_size=16, stride=16)  # 卷积
2. x.flatten(2)                                # 展平
3. x.transpose(1, 2)                           # 转置
```

**练习**: 用纸笔画出 (2,3,224,224) 如何变成 (2,196,768)

---

#### Step 2: MLP (第二简单)
```python
# 就是两层 Linear + GELU
fc1(768 → 3072) → GELU → fc2(3072 → 768)
```

**练习**: 计算 MLP 的参数量
- fc1: 768 × 3072 = ?
- fc2: 3072 × 768 = ?

---

#### Step 3: Attention (最难，慢慢来)

**难点在维度变换**，我帮你拆解：

```python
# 输入: x (B, N, C) = (2, 197, 768)

# 【步骤 1】生成 QKV
qkv = self.qkv(x)  # (2, 197, 768) → (2, 197, 2304)
# 为什么是 2304？因为 768*3 = 2304 (Q+K+V)

# 【步骤 2】重塑 - 关键！
qkv = qkv.reshape(B, N, 3, 12, 64)
#                     ↑  ↑  ↑
#              3个矩阵 12头 每头64维

# 【步骤 3】转置 - 方便后续计算
qkv = qkv.permute(2, 0, 3, 1, 4)
# 变成 (3, B, 12, N, 64)
#      ↑        ↑
#   QKV分开   多头维度提前

# 【步骤 4】分离 Q, K, V
q, k, v = qkv[0], qkv[1], qkv[2]
# 每个都是 (B, 12, 197, 64)

# 【步骤 5】计算注意力
attn = (q @ k.transpose(-2, -1)) * self.scale
# q: (B,12,197,64) @ k^T: (B,12,64,197) = (B,12,197,197)
#                                              ↑     ↑
#                                           from   to
# attn[i,j] = "token i 对 token j 的权重"

attn = attn.softmax(dim=-1)  # 每一行和为 1

# 【步骤 6】加权求和
x = attn @ v
# (B,12,197,197) @ (B,12,197,64) = (B,12,197,64)

# 【步骤 7】合并多头
x = x.transpose(1, 2)        # (B,197,12,64)
x = x.reshape(B, N, C)       # (B,197,768)
```

**练习**:
- 用纸笔画出 (2,197,768) → (2,12,197,64) 的变换
- 理解 `attn[b,h,i,j]` 每个维度的含义

**调试技巧**:
```python
# 在每一步后打印形状
print(f"qkv: {qkv.shape}")
print(f"q: {q.shape}")
print(f"attn: {attn.shape}")
```

---

#### Step 4: Block (组合)
```python
# 只是把 Attention 和 MLP 组合起来
x = x + self.attn(self.norm1(x))  # 第一个子层
x = x + self.mlp(self.norm2(x))   # 第二个子层
```

**注意**: Pre-Norm（先 Norm 再操作）

---

#### Step 5: VisionTransformer (串联)
```python
# 把所有模块串起来
1. Patch Embedding
2. 加 CLS token
3. 加位置编码
4. 通过 12 层 Transformer
5. 取 CLS token
6. 分类头
```

**练习**: 对着我的代码，把 `forward()` 函数手写一遍

---

### 阶段 3: 可视化和训练（第三天）

**任务**:
- [ ] 可视化 Attention Map（看模型在看哪里）
- [ ] 在 CIFAR-10 上训练一个小模型
- [ ] 用 Wandb 离线模式记录训练曲线

我会提供对应的代码！

---

## 🤔 关键问题自测

### 基础理解
1. ViT 把图像当作什么？（序列）
2. 一张 224×224 的图像会被切成多少个 patches？（196）
3. 每个 patch 投影到多少维？（768）
4. CLS token 的作用是什么？（聚合全局信息用于分类）

### Attention 机制
1. Q、K、V 分别是什么意思？
   - Q (Query): 我要查询什么
   - K (Key): 我有什么信息
   - V (Value): 我的信息内容

2. 为什么要除以 √d？
   - 防止点积过大导致 softmax 梯度消失

3. 多头注意力的优势？
   - 每个头学习不同的模式（边缘、纹理、全局）

### 架构细节
1. Pre-Norm vs Post-Norm 的区别？
   - Pre-Norm: LayerNorm 在子层之前（ViT 用这个）
   - Post-Norm: LayerNorm 在残差之后（原始 Transformer）

2. 为什么 ViT 在小数据集上不如 CNN？
   - CNN 有归纳偏置（局部性、平移不变性）
   - ViT 几乎没有归纳偏置，需要大规模数据学习

3. ViT-Base 有多少参数？
   - ~86M

---

## 💡 学习技巧

### 1. 不要一次性理解所有细节
- 第一遍：看懂整体流程
- 第二遍：理解每个模块
- 第三遍：关注实现细节

### 2. 画图！画图！画图！
- 用纸笔画维度变化
- 画 Attention 的计算流程
- 画整体架构图

### 3. 动手实验
```python
# 创建一个小的 tensor 实验
x = torch.randn(2, 3, 4)  # 小数据
print(x.shape)

# 试试 reshape
x2 = x.reshape(2, 3, 2, 2)
print(x2.shape)

# 试试 transpose
x3 = x.transpose(1, 2)
print(x3.shape)
```

### 4. 对比 CNN
| 特性 | CNN | ViT |
|------|-----|-----|
| 输入 | 图像 → 卷积 | 图像 → Patches → Tokens |
| 感受野 | 局部 → 全局（堆叠） | 第一层就全局 |
| 归纳偏置 | 强（局部性、平移不变） | 弱 |
| 数据需求 | 小数据集可以 | 需要大规模预训练 |

---

## 📚 额外资源

### 论文
- 原文: An Image is Worth 16x16 Words
- 位置: `~/桌面/vison_transformer.pdf`
- **必看**: 第 1-7 页（你已经翻译过了）

### 博客推荐
- The Illustrated Transformer (Jay Alammar)
- Attention Is All You Need 论文解读

### 代码参考
- 官方 JAX 实现: google-research/vision_transformer
- PyTorch 实现 (timm): huggingface/pytorch-image-models

---

## 🎯 下一步计划

### 短期（1 周内）
- [ ] 完全理解代码（今天）
- [ ] 手写一遍代码（明天）
- [ ] 可视化 Attention（后天）
- [ ] 训练一个小模型

### 中期（1 个月）
- [ ] 在自己的数据集上训练
- [ ] 实现一些变体（Swin Transformer, PVT）
- [ ] 阅读后续工作（DeiT, BEiT, MAE）

### 长期（科研方向）
- [ ] 把 ViT 用到 VLA 中（视觉编码器）
- [ ] 探索 Transformer 在灵巧手控制中的应用
- [ ] 发第一篇论文！

---

## ❓ 遇到问题怎么办

### 代码问题
1. **维度不匹配**
   - 打印每一步的 shape
   - 对照我的代码检查

2. **不理解某个操作**
   - 创建小 tensor 实验
   - 查 PyTorch 文档

3. **数学推导不懂**
   - 看 vit_architecture.md
   - 用具体数字代入计算

### 概念问题
1. 先看 README.md 的对应章节
2. 再看 vit_architecture.md 的详细解释
3. 还不懂就问我！

---

## 🌟 你已经完成的

1. ✅ 翻译了 ViT 论文前 7 页
2. ✅ 理解了 CNN vs ViT 的区别
3. ✅ 理解了归纳偏置的概念
4. ✅ 看懂了整体架构
5. ✅ 有了完整的代码参考

**你的基础很好！接下来就是动手实践了** 💪

---

## 📝 学习记录

### 2025-12-07
- [x] 创建项目结构
- [x] 编写完整代码
- [x] 运行测试（全部通过）
- [x] 理解 Wandb 的作用

### 下次学习（记得更新）
- [ ] 手写 PatchEmbed
- [ ] 手写 Attention
- [ ] ...

---

**加油！你可以的！🚀**

有任何问题随时问我，我会非常详细地解答！

记住：**代码能力是练出来的，不是天生的**。你现在的起点已经很好了（理解能力强），多写几遍就会越来越熟练 😊
