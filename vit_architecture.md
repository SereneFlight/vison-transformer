# ViT æ¶æ„è¯¦è§£ï¼šæ•°å­¦åŸç† + ç»´åº¦å˜åŒ–

> è¿™ä¸ªæ–‡æ¡£ä¼š**éå¸¸è¯¦ç»†**åœ°è§£é‡Šæ¯ä¸€æ­¥çš„æ•°å­¦åŸç†å’Œç»´åº¦å˜åŒ–
> é€‚åˆä»£ç èƒ½åŠ›è¿˜åœ¨æå‡çš„åŒå­¦ä»”ç»†é˜…è¯»ï¼

---

## ç›®å½•
1. [æ•´ä½“æµç¨‹](#1-æ•´ä½“æµç¨‹)
2. [Patch Embedding è¯¦è§£](#2-patch-embedding-è¯¦è§£)
3. [Multi-Head Attention è¯¦è§£](#3-multi-head-attention-è¯¦è§£)
4. [MLP è¯¦è§£](#4-mlp-è¯¦è§£)
5. [Transformer Block è¯¦è§£](#5-transformer-block-è¯¦è§£)
6. [å®Œæ•´æ¨¡å‹è¯¦è§£](#6-å®Œæ•´æ¨¡å‹è¯¦è§£)
7. [è®­ç»ƒç»†èŠ‚](#7-è®­ç»ƒç»†èŠ‚)

---

## 1. æ•´ä½“æµç¨‹

### 1.1 è¾“å…¥è¾“å‡º
```
è¾“å…¥ï¼šRGB å›¾åƒ
- å½¢çŠ¶ï¼š(Batch, 3, 224, 224)
- æ•°å€¼èŒƒå›´ï¼š[0, 1] æˆ– [-1, 1]ï¼ˆå½’ä¸€åŒ–åï¼‰

è¾“å‡ºï¼šç±»åˆ«æ¦‚ç‡
- å½¢çŠ¶ï¼š(Batch, num_classes)
- ä¾‹å¦‚ ImageNet-1Kï¼š(Batch, 1000)
```

### 1.2 æ•°æ®æµ

```
(B, 3, 224, 224)                    è¾“å…¥å›¾åƒ
    â†“
(B, 196, 768)                       Patch Embedding
    â†“
(B, 197, 768)                       åŠ  CLS token
    â†“
(B, 197, 768)                       åŠ ä½ç½®ç¼–ç 
    â†“
(B, 197, 768) â†’ Block 1 â†’ ...       Transformer Encoder
    â†“
(B, 197, 768)                       12å±‚å
    â†“
(B, 768)                            å– CLS token
    â†“
(B, num_classes)                    åˆ†ç±»å¤´
```

---

## 2. Patch Embedding è¯¦è§£

### 2.1 åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠå›¾åƒå½“ä½œä¸€ä¸ª"å¥å­"ï¼Œæ¯ä¸ª patch æ˜¯ä¸€ä¸ª"å•è¯"

```
åŸå§‹å›¾åƒ: 224Ã—224Ã—3
    â†“
åˆ‡åˆ†æˆ patches: (224/16) Ã— (224/16) = 14Ã—14 = 196 ä¸ª
æ¯ä¸ª patch: 16Ã—16Ã—3 = 768 ä¸ªåƒç´ å€¼
    â†“
å±•å¹³æ¯ä¸ª patch: [768]
    â†“
çº¿æ€§æŠ•å½±: [768] â†’ [768]  (å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ)
```

### 2.2 ä¸ºä»€ä¹ˆç”¨å·ç§¯å®ç°ï¼Ÿ

**æ•°å­¦ç­‰ä»·æ€§è¯æ˜**ï¼š

æ–¹æ³•1ï¼ˆåŸå§‹æè¿°ï¼‰ï¼š
```python
# 1. åˆ‡åˆ† patch
patches = []
for i in range(14):
    for j in range(14):
        patch = img[:, :, i*16:(i+1)*16, j*16:(j+1)*16]  # (B, 3, 16, 16)
        patch = patch.reshape(B, -1)  # (B, 768)
        patches.append(patch)
patches = torch.stack(patches, dim=1)  # (B, 196, 768)

# 2. çº¿æ€§æŠ•å½±
output = patches @ W  # W: (768, 768)
```

æ–¹æ³•2ï¼ˆå·ç§¯å®ç°ï¼‰ï¼š
```python
# ä¸€æ­¥åˆ°ä½ï¼
output = Conv2d(3, 768, kernel_size=16, stride=16)(img)
output = output.flatten(2).transpose(1, 2)
```

**ä¸ºä»€ä¹ˆç­‰ä»·ï¼Ÿ**
- å·ç§¯çš„ kernel_size=16, stride=16 â†’ æ­£å¥½ä¸é‡å åœ°æ‰«è¿‡æ•´ä¸ªå›¾åƒ
- æ¯ä¸ªå·ç§¯æ ¸çš„è¾“å‡º â†’ å¯¹åº”ä¸€ä¸ª patch çš„çº¿æ€§æŠ•å½±
- è®¡ç®—æ•ˆç‡æ›´é«˜ï¼ŒGPU å‹å¥½ï¼

### 2.3 è¯¦ç»†ç»´åº¦å˜åŒ–

```python
è¾“å…¥: x
å½¢çŠ¶: (B, 3, 224, 224)
    â†“
ã€å·ç§¯ã€‘self.proj = Conv2d(3, 768, kernel_size=16, stride=16)
    å‚æ•°é‡: 3 Ã— 768 Ã— 16 Ã— 16 = 589,824
    â†“
è¾“å‡º: (B, 768, 14, 14)
    è§£é‡Š: 768 ä¸ªé€šé“ï¼Œ14Ã—14 çš„ç©ºé—´ä½ç½®
    â†“
ã€å±•å¹³ã€‘x.flatten(2)
    flatten ä»ç¬¬2ç»´å¼€å§‹ï¼ˆ0=B, 1=C, 2=Hï¼‰
    â†“
è¾“å‡º: (B, 768, 196)
    è§£é‡Š: 196 = 14Ã—14
    â†“
ã€è½¬ç½®ã€‘x.transpose(1, 2)
    äº¤æ¢ç»´åº¦ 1 å’Œ 2
    â†“
æœ€ç»ˆè¾“å‡º: (B, 196, 768)
    è§£é‡Š: 196 ä¸ª tokensï¼Œæ¯ä¸ª 768 ç»´
```

### 2.4 ä»£ç å®ç°

```python
class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2  # 196

        # ç”¨å·ç§¯å®ç° patch embedding
        self.proj = nn.Conv2d(
            in_chans,      # 3
            embed_dim,     # 768
            kernel_size=patch_size,  # 16
            stride=patch_size        # 16
        )

    def forward(self, x):
        # x: (B, 3, 224, 224)
        x = self.proj(x)            # (B, 768, 14, 14)
        x = x.flatten(2)            # (B, 768, 196)
        x = x.transpose(1, 2)       # (B, 196, 768)
        return x
```

---

## 3. Multi-Head Attention è¯¦è§£

### 3.1 Self-Attention åŸç†

**ç›®æ ‡**ï¼šè®©æ¯ä¸ª token å…³æ³¨å…¶ä»–æ‰€æœ‰ token

**ä¸‰ä¸ªå…³é”®çŸ©é˜µ**ï¼š
- **Q (Query)**ï¼šæˆ‘è¦æŸ¥è¯¢ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
- **K (Key)**ï¼šæˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
- **V (Value)**ï¼šæˆ‘çš„ä¿¡æ¯å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ

**ç±»æ¯”æœç´¢å¼•æ“**ï¼š
```
ä½ è¾“å…¥ "Vision Transformer è®ºæ–‡"  â† Query
æœç´¢å¼•æ“çš„ç´¢å¼•åº“                â† Key
è¿”å›çš„è®ºæ–‡å†…å®¹                  â† Value

åŒ¹é…åº¦ = Query Â· Key
ç»“æœ = Î£ (åŒ¹é…åº¦ Ã— Value)
```

### 3.2 å…¬å¼æ¨å¯¼

**Step 1: è®¡ç®— Q, K, V**
```
è¾“å…¥: X âˆˆ â„^(NÃ—D)  # N=197 tokens, D=768 ç»´åº¦

Q = XW_q,  W_q âˆˆ â„^(DÃ—D)  â†’  Q âˆˆ â„^(NÃ—D)
K = XW_k,  W_k âˆˆ â„^(DÃ—D)  â†’  K âˆˆ â„^(NÃ—D)
V = XW_v,  W_v âˆˆ â„^(DÃ—D)  â†’  V âˆˆ â„^(NÃ—D)
```

**Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**
```
Scores = QK^T / âˆšd_k

ç»´åº¦åˆ†æ:
Q: (N, D) Ã— K^T: (D, N) = (N, N)

æ¯ä¸ªå…ƒç´  scores[i, j] è¡¨ç¤º:
"token i å¯¹ token j çš„å…³æ³¨ç¨‹åº¦"
```

**ä¸ºä»€ä¹ˆé™¤ä»¥ âˆšd_kï¼Ÿ**
```
QK^T çš„å€¼ä¼šéšç€ç»´åº¦å¢å¤§è€Œå¢å¤§
ä¾‹å¦‚: d=768 æ—¶ï¼Œç‚¹ç§¯å¯èƒ½è¾¾åˆ°å‡ ç™¾
softmax(å¤§æ•°) â†’ æ¢¯åº¦æ¶ˆå¤±ï¼

é™¤ä»¥ âˆš768 â‰ˆ 27.7 â†’ ç¼©æ”¾åˆ°åˆç†èŒƒå›´
```

**Step 3: Softmax å½’ä¸€åŒ–**
```
Attention = softmax(Scores, dim=-1)

æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ:
Î£_j attention[i, j] = 1

attention[i, j] = "token i ç»™ token j çš„æƒé‡"
```

**Step 4: åŠ æƒæ±‚å’Œ**
```
Output = Attention Ã— V

ç»´åº¦: (N, N) Ã— (N, D) = (N, D)

output[i] = Î£_j attention[i, j] * v[j]
         = "æŠŠæ‰€æœ‰ token çš„ value æŒ‰æ³¨æ„åŠ›æƒé‡åŠ æƒå¹³å‡"
```

### 3.3 Multi-Head çš„æ„ä¹‰

**å•å¤´çš„å±€é™**ï¼š
- åªèƒ½å­¦ä¹ ä¸€ç§æ¨¡å¼
- æ¯”å¦‚åªå…³æ³¨ç©ºé—´ä½ç½®ï¼Œå¿½ç•¥è¯­ä¹‰ä¿¡æ¯

**å¤šå¤´çš„ä¼˜åŠ¿**ï¼š
- 12 ä¸ªå¤´ = 12 ç§ä¸åŒçš„å…³æ³¨æ¨¡å¼
- å¤´1: å…³æ³¨è¾¹ç¼˜
- å¤´2: å…³æ³¨çº¹ç†
- å¤´3: å…³æ³¨é¢œè‰²
- ...

**å®ç°æ–¹å¼**ï¼š
```
åŸå§‹ç»´åº¦: D = 768
å¤´æ•°: h = 12
æ¯ä¸ªå¤´çš„ç»´åº¦: d_h = D/h = 64

å¯¹äºæ¯ä¸ªå¤´ i:
Q_i = X W_q^i,  W_q^i âˆˆ â„^(768Ã—64)  â†’  Q_i âˆˆ â„^(NÃ—64)
K_i = X W_k^i,  W_k^i âˆˆ â„^(768Ã—64)  â†’  K_i âˆˆ â„^(NÃ—64)
V_i = X W_v^i,  W_v^i âˆˆ â„^(768Ã—64)  â†’  V_i âˆˆ â„^(NÃ—64)

head_i = Attention(Q_i, K_i, V_i)  â†’  â„^(NÃ—64)

æœ€åæ‹¼æ¥:
Output = Concat(head_1, ..., head_12)  â†’  â„^(NÃ—768)
```

### 3.4 è¯¦ç»†ç»´åº¦å˜åŒ–

```python
è¾“å…¥: x
å½¢çŠ¶: (B, 197, 768)
    â†“
ã€ç”Ÿæˆ QKVã€‘self.qkv = Linear(768, 768*3)
    â†“
qkv: (B, 197, 2304)  # 2304 = 768*3
    â†“
ã€é‡å¡‘ã€‘qkv.reshape(B, N, 3, num_heads, head_dim)
    â†“
qkv: (B, 197, 3, 12, 64)
    â†“
ã€è½¬ç½®ã€‘qkv.permute(2, 0, 3, 1, 4)
    â†“
qkv: (3, B, 12, 197, 64)
    â†“
ã€åˆ†ç¦»ã€‘q, k, v = qkv[0], qkv[1], qkv[2]
    â†“
q: (B, 12, 197, 64)
k: (B, 12, 197, 64)
v: (B, 12, 197, 64)
    â†“
ã€è®¡ç®—æ³¨æ„åŠ›ã€‘attn = (q @ k.T) * scale
    q @ k.T: (B, 12, 197, 64) Ã— (B, 12, 64, 197)
    â†“
attn: (B, 12, 197, 197)  # æ³¨æ„åŠ›çŸ©é˜µ
    â†“
ã€softmaxã€‘attn = softmax(attn, dim=-1)
    â†“
ã€åŠ æƒæ±‚å’Œã€‘x = attn @ v
    (B, 12, 197, 197) Ã— (B, 12, 197, 64)
    â†“
x: (B, 12, 197, 64)
    â†“
ã€è½¬ç½®ã€‘x.transpose(1, 2)
    â†“
x: (B, 197, 12, 64)
    â†“
ã€åˆå¹¶å¤šå¤´ã€‘x.reshape(B, N, -1)
    â†“
x: (B, 197, 768)
    â†“
ã€è¾“å‡ºæŠ•å½±ã€‘self.proj(x)
    â†“
æœ€ç»ˆè¾“å‡º: (B, 197, 768)
```

### 3.5 ä»£ç å®ç°

```python
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads  # 768 / 12 = 64
        self.scale = self.head_dim ** -0.5  # 1/âˆš64

        # ä¸€æ¬¡æ€§ç”Ÿæˆ Q, K, V
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)

        # è¾“å‡ºæŠ•å½±
        self.proj = nn.Linear(dim, dim)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape  # (2, 197, 768)

        # ç”Ÿæˆ QKV
        qkv = self.qkv(x)  # (B, 197, 2304)
        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, 12, 197, 64)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # è®¡ç®—æ³¨æ„åŠ›
        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, 12, 197, 197)
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # åŠ æƒæ±‚å’Œ
        x = attn @ v  # (B, 12, 197, 64)

        # åˆå¹¶å¤šå¤´
        x = x.transpose(1, 2)  # (B, 197, 12, 64)
        x = x.reshape(B, N, C)  # (B, 197, 768)

        # è¾“å‡ºæŠ•å½±
        x = self.proj(x)
        x = self.proj_drop(x)

        return x
```

---

## 4. MLP è¯¦è§£

### 4.1 åŸç†

**ä½œç”¨**ï¼šå¯¹æ¯ä¸ª token ç‹¬ç«‹åœ°åšéçº¿æ€§å˜æ¢

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- Attention æ˜¯çº¿æ€§æ“ä½œï¼ˆåŠ æƒå¹³å‡ï¼‰
- éœ€è¦ MLP å¼•å…¥éçº¿æ€§ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›

### 4.2 ç»“æ„

```
è¾“å…¥: (B, 197, 768)
    â†“
Linear(768 â†’ 3072)      # æ‰©å¤§ 4 å€
    â†“
GELU æ¿€æ´»
    â†“
Dropout(0.1)
    â†“
Linear(3072 â†’ 768)      # å‹ç¼©å›æ¥
    â†“
Dropout(0.1)
    â†“
è¾“å‡º: (B, 197, 768)
```

### 4.3 GELU æ¿€æ´»å‡½æ•°

**å…¬å¼**ï¼š
```
GELU(x) = x Â· Î¦(x)
Î¦(x) = æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

è¿‘ä¼¼:
GELU(x) â‰ˆ 0.5x(1 + tanh[âˆš(2/Ï€)(x + 0.044715xÂ³)])
```

**å¯¹æ¯” ReLU**ï¼š
```
ReLU(x) = max(0, x)
    â†‘ ç¡¬æˆªæ–­ï¼Œx<0 æ—¶æ¢¯åº¦ä¸º 0

GELU(x) = x Â· Î¦(x)
    â†‘ å¹³æ»‘ç‰ˆæœ¬ï¼Œæ‰€æœ‰åœ°æ–¹å¯å¯¼
    â†‘ Transformer æ ‡é…ï¼
```

### 4.4 ä»£ç å®ç°

```python
class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None,
                 act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1 = nn.Linear(in_features, hidden_features)  # 768 â†’ 3072
        self.act = act_layer()  # GELU
        self.drop1 = nn.Dropout(drop)
        self.fc2 = nn.Linear(hidden_features, out_features)  # 3072 â†’ 768
        self.drop2 = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x
```

---

## 5. Transformer Block è¯¦è§£

### 5.1 Pre-Norm vs Post-Norm

**Post-Normï¼ˆåŸå§‹ Transformerï¼‰**ï¼š
```
x = LayerNorm(x + Attention(x))
x = LayerNorm(x + MLP(x))
```

**Pre-Normï¼ˆViT ä½¿ç”¨ï¼‰**ï¼š
```
x = x + Attention(LayerNorm(x))
x = x + MLP(LayerNorm(x))
```

**åŒºåˆ«**ï¼š
- Pre-Norm: å…ˆå½’ä¸€åŒ–å†åšå­å±‚æ“ä½œ
- Post-Norm: å…ˆåšå­å±‚æ“ä½œå†å½’ä¸€åŒ–

**ä¸ºä»€ä¹ˆ ViT ç”¨ Pre-Normï¼Ÿ**
- è®­ç»ƒæ›´ç¨³å®šï¼ˆæ¢¯åº¦æµæ›´é¡ºç•…ï¼‰
- å¯ä»¥ä¸ç”¨å­¦ä¹ ç‡ warmup
- æ·±å±‚ç½‘ç»œï¼ˆå¦‚ ViT-Huge 32å±‚ï¼‰ä¹Ÿèƒ½è®­ç»ƒ

### 5.2 æ®‹å·®è¿æ¥çš„ä½œç”¨

```
x_out = x_in + F(x_in)
```

**å¥½å¤„**ï¼š
1. **æ¢¯åº¦æµç•…**ï¼šåå‘ä¼ æ’­æ—¶æ¢¯åº¦å¯ä»¥ç›´æ¥å›ä¼ 
2. **æ’ç­‰æ˜ å°„**ï¼šè‡³å°‘å¯ä»¥å­¦åˆ° F(x)=0ï¼Œä¸ä¼šé€€åŒ–
3. **æ·±å±‚ç½‘ç»œ**ï¼šå¯ä»¥å †å æ›´å¤šå±‚

### 5.3 å®Œæ•´æµç¨‹

```python
è¾“å…¥: x (B, 197, 768)

# ç¬¬ä¸€ä¸ªå­å±‚ï¼šSelf-Attention
norm_x = LayerNorm(x)           # (B, 197, 768)
attn_out = Attention(norm_x)    # (B, 197, 768)
x = x + attn_out                # æ®‹å·®è¿æ¥

# ç¬¬äºŒä¸ªå­å±‚ï¼šMLP
norm_x = LayerNorm(x)           # (B, 197, 768)
mlp_out = MLP(norm_x)           # (B, 197, 768)
x = x + mlp_out                 # æ®‹å·®è¿æ¥

è¾“å‡º: x (B, 197, 768)
```

### 5.4 ä»£ç å®ç°

```python
class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,
                 drop=0., attn_drop=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,
                              attn_drop=attn_drop, proj_drop=drop)

        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)  # 768 * 4 = 3072
        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=nn.GELU, drop=drop)

    def forward(self, x):
        # æ³¨æ„åŠ›å­å±‚
        x = x + self.attn(self.norm1(x))

        # MLP å­å±‚
        x = x + self.mlp(self.norm2(x))

        return x
```

---

## 6. å®Œæ•´æ¨¡å‹è¯¦è§£

### 6.1 CLS Token

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- å€Ÿé‰´ BERT çš„ [CLS] token
- æ‰€æœ‰ patch tokens é€šè¿‡ self-attention äº¤äº’
- CLS token èšåˆå…¨å±€ä¿¡æ¯
- æœ€ååªç”¨ CLS token åšåˆ†ç±»

**å®ç°**ï¼š
```python
# å¯å­¦ä¹ å‚æ•°
self.cls_token = nn.Parameter(torch.zeros(1, 1, 768))

# forward æ—¶ï¼š
cls_token = self.cls_token.expand(B, -1, -1)  # (1,1,768) â†’ (B,1,768)
x = torch.cat([cls_token, x], dim=1)  # (B, 196, 768) â†’ (B, 197, 768)
```

### 6.2 Position Embedding

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- Self-Attention æ˜¯**ç½®æ¢ä¸å˜**çš„
- æ‰“ä¹± token é¡ºåºï¼Œè¾“å‡ºä¸å˜ï¼
- éœ€è¦ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹"ä½ æ˜¯ç¬¬å‡ ä¸ª patch"

**ViT çš„é€‰æ‹©**ï¼šå¯å­¦ä¹ çš„ç»å¯¹ä½ç½®ç¼–ç 
```python
self.pos_embed = nn.Parameter(torch.zeros(1, 197, 768))

# forward æ—¶ï¼š
x = x + self.pos_embed  # å¹¿æ’­åŠ æ³•
```

**å…¶ä»–é€‰æ‹©**ï¼š
- æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆåŸå§‹ Transformerï¼‰
- ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆT5, DeBERTaï¼‰
- æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoFormerï¼‰

### 6.3 å®Œæ•´å‰å‘ä¼ æ’­

```python
def forward(self, x):
    B = x.shape[0]

    # 1. Patch Embedding
    x = self.patch_embed(x)  # (B,3,224,224) â†’ (B,196,768)

    # 2. åŠ  CLS token
    cls_token = self.cls_token.expand(B, -1, -1)  # (B,1,768)
    x = torch.cat([cls_token, x], dim=1)  # (B,197,768)

    # 3. åŠ ä½ç½®ç¼–ç 
    x = x + self.pos_embed  # (B,197,768)
    x = self.pos_drop(x)

    # 4. é€šè¿‡ 12 å±‚ Transformer
    for block in self.blocks:
        x = block(x)  # (B,197,768) â†’ (B,197,768)

    # 5. æœ€åçš„ Layer Norm
    x = self.norm(x)  # (B,197,768)

    # 6. åªå– CLS token
    cls_output = x[:, 0]  # (B,768)

    # 7. åˆ†ç±»å¤´
    logits = self.head(cls_output)  # (B,768) â†’ (B,num_classes)

    return logits
```

### 6.4 å‚æ•°é‡è®¡ç®—

**ViT-Base/16**ï¼š
```
1. Patch Embedding:
   - Conv2d: 3Ã—768Ã—16Ã—16 = 589,824

2. CLS token:
   - 1Ã—1Ã—768 = 768

3. Position Embedding:
   - 1Ã—197Ã—768 = 151,296

4. Transformer Block Ã— 12:
   æ¯ä¸ª Block:
   - Attention:
     * QKV: 768Ã—(768Ã—3) = 1,769,472
     * Proj: 768Ã—768 = 589,824
   - MLP:
     * fc1: 768Ã—3072 = 2,359,296
     * fc2: 3072Ã—768 = 2,359,296
   - LayerNorm Ã— 2: å¿½ç•¥ä¸è®¡

   å•ä¸ª Block: ~7M å‚æ•°
   12 ä¸ª Block: ~84M å‚æ•°

5. æœ€åçš„ LayerNorm: 768

6. åˆ†ç±»å¤´:
   - Linear: 768Ã—1000 = 768,000

æ€»è®¡: ~86M å‚æ•°
```

---

## 7. è®­ç»ƒç»†èŠ‚

### 7.1 é¢„è®­ç»ƒç­–ç•¥

**æ•°æ®é›†**ï¼š
- å°æ•°æ®ï¼šImageNet-1K (1.2M å›¾åƒ)
- ä¸­æ•°æ®ï¼šImageNet-21K (14M å›¾åƒ)
- å¤§æ•°æ®ï¼šJFT-300M (300M å›¾åƒ)

**è®ºæ–‡å‘ç°**ï¼š
```
ImageNet-1K é¢„è®­ç»ƒ:
  ViT-Base < ResNet-50  âŒ

ImageNet-21K é¢„è®­ç»ƒ:
  ViT-Base â‰ˆ ResNet-101  âœ“

JFT-300M é¢„è®­ç»ƒ:
  ViT-Base > ResNet-152  âœ“âœ“
```

### 7.2 è¶…å‚æ•°

```python
# ä¼˜åŒ–å™¨
optimizer = Adam(lr=0.001, betas=(0.9, 0.999), weight_decay=0.1)

# å­¦ä¹ ç‡è°ƒåº¦
# 1. Warmup: 0 â†’ 0.001 (10k steps)
# 2. Cosine decay: 0.001 â†’ 0 (å‰©ä½™ steps)

# æ­£åˆ™åŒ–
dropout = 0.1
stochastic_depth = 0.1  # éšæœºä¸¢å¼ƒæ•´ä¸ª Block

# æ•°æ®å¢å¼º
- RandAugment
- Mixup
- Cutmix
- Random Erasing
```

### 7.3 Fine-tuning ç»†èŠ‚

```
é¢„è®­ç»ƒæ¨¡å‹: åœ¨ ImageNet-21K (14M å›¾åƒ) ä¸Š
    â†“
Fine-tune: åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Š
    - åˆ†è¾¨ç‡: 224 â†’ 384 (å¯é€‰)
    - å­¦ä¹ ç‡: 0.001 â†’ 0.003
    - Batch size: 512
    - Epochs: ~20
```

**ä½ç½®ç¼–ç çš„å¤„ç†**ï¼š
- é¢„è®­ç»ƒ: 224Ã—224 â†’ 14Ã—14 = 196 patches
- Fine-tune: 384Ã—384 â†’ 24Ã—24 = 576 patches
- è§£å†³æ–¹æ¡ˆ: 2D æ’å€¼ (bicubic)

---

## 8. é‡è¦å›¾è¡¨

### 8.1 Attention Map ç¤ºä¾‹

```
è®ºæ–‡ Figure 7:

ç¬¬1å±‚: å…³æ³¨å±€éƒ¨é‚»åŸŸï¼ˆç±»ä¼¼ CNNï¼‰
ç¬¬6å±‚: å¼€å§‹å…³æ³¨å…¨å±€ç»“æ„
ç¬¬12å±‚: èšç„¦åˆ°ç›®æ ‡ç‰©ä½“

ä¸åŒçš„å¤´å­¦åˆ°ä¸åŒçš„æ¨¡å¼:
- å¤´1: è¾¹ç¼˜æ£€æµ‹
- å¤´2: çº¹ç†æ¨¡å¼
- å¤´3: å…¨å±€å½¢çŠ¶
```

### 8.2 æ€§èƒ½å¯¹æ¯”

```
ImageNet-1K (ä»å¤´è®­ç»ƒ):
  ResNet-50:     76.5%
  ViT-Base:      77.9%  â† ç•¥å¥½

ImageNet-21K é¢„è®­ç»ƒ â†’ ImageNet-1K:
  ResNet-152:    78.3%
  ViT-Base:      81.8%  â† æ˜¾è‘—æå‡

JFT-300M é¢„è®­ç»ƒ â†’ ImageNet-1K:
  ResNet-152:    79.8%
  ViT-Huge:      88.5%  â† ç¢¾å‹å¼é¢†å…ˆ
```

---

## 9. å¸¸è§é”™è¯¯å’Œè°ƒè¯•æŠ€å·§

### 9.1 ç»´åº¦ä¸åŒ¹é…

```python
# é”™è¯¯ç¤ºä¾‹
x = x.reshape(B, N, C)  # å¿˜è®°è€ƒè™‘ num_heads

# æ­£ç¡®åšæ³•
x = x.transpose(1, 2).reshape(B, N, self.num_heads * self.head_dim)
```

### 9.2 æ³¨æ„åŠ›åˆ†æ•°æº¢å‡º

```python
# é—®é¢˜: attn = softmax(q @ k.T) æ¢¯åº¦æ¶ˆå¤±

# è§£å†³: ç¼©æ”¾
attn = softmax((q @ k.T) / sqrt(d_k))
```

### 9.3 è°ƒè¯•å»ºè®®

```python
# åœ¨æ¯ä¸ªæ¨¡å—åæ‰“å°å½¢çŠ¶
print(f"After patch_embed: {x.shape}")
print(f"After add cls: {x.shape}")
print(f"After block 0: {x.shape}")
...

# æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm()}")
```

---

**è¿™ä»½æ–‡æ¡£åº”è¯¥èƒ½å¸®ä½ å®Œå…¨ç†è§£ ViT çš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼**

æœ‰ä»»ä½•ä¸æ‡‚çš„åœ°æ–¹ï¼Œéšæ—¶é—®æˆ‘ ğŸ˜Š
