# ViT CIFAR-10 训练说明

## 🎯 训练配置

### 数据集
- **CIFAR-10**: 10个类别的彩色图像
  - 训练集: 50,000 张
  - 测试集: 10,000 张
  - 图像大小: 32×32 (会被 resize 到 224×224)

### 类别
```
0. 飞机 (airplane)
1. 汽车 (automobile)
2. 鸟 (bird)
3. 猫 (cat)
4. 鹿 (deer)
5. 狗 (dog)
6. 青蛙 (frog)
7. 马 (horse)
8. 船 (ship)
9. 卡车 (truck)
```

### 模型配置（小版本 ViT）
```python
img_size = 224        # 输入图像大小
patch_size = 16       # Patch 大小
embed_dim = 384       # 嵌入维度（ViT-Base 是 768）
depth = 6             # Transformer 层数（ViT-Base 是 12）
num_heads = 6         # 注意力头数（ViT-Base 是 12）
参数量: 约 22M        # ViT-Base 是 86M
```

**为什么用小版本？**
- ✅ 训练更快（适合快速验证）
- ✅ 显存需求小（笔记本 GPU 也能跑）
- ✅ CIFAR-10 数据集较小，大模型容易过拟合

### 训练超参数
```python
batch_size = 64
num_epochs = 20
learning_rate = 3e-4
weight_decay = 0.1
optimizer = AdamW
scheduler = CosineAnnealingLR
```

### 数据增强
- Resize: 32×32 → 224×224
- RandomHorizontalFlip: 随机水平翻转
- RandomCrop: 随机裁剪（padding=4）
- Normalize: 使用 CIFAR-10 的均值和标准差

---

## 📊 预期结果

### 训练时间（RTX 5070 Laptop）
- 每个 epoch: 约 1-2 分钟
- 总训练时间: 约 20-40 分钟

### 预期准确率
```
Epoch 1:   约 30-40% (随机初始化，还在学习)
Epoch 5:   约 50-60% (开始收敛)
Epoch 10:  约 65-75% (性能提升)
Epoch 20:  约 70-80% (接近收敛)
```

**注意**：
- CIFAR-10 是 32×32 的小图像，resize 到 224×224 会损失信息
- 从零训练（没有预训练），准确率不会特别高
- ViT-Base (12层, 768维) 在 ImageNet 预训练后 fine-tune 可以达到 90%+

---

## 📂 文件说明

### 训练脚本
```
train_cifar10.py       # 主训练脚本
vit_model.py           # ViT 模型定义
```

### 自动生成的文件
```
data/                  # CIFAR-10 数据集（自动下载）
checkpoints/           # 模型保存目录
  └── best_model.pth   # 最佳模型权重
```

---

## 🚀 如何运行

### 第一次运行（会自动下载数据）
```bash
cd ~/桌面/vla_learning/vit_from_scratch
python3 train_cifar10.py
```

### 第二次运行（数据已下载）
```bash
python3 train_cifar10.py
```

### 停止训练
```
Ctrl + C
```

---

## 📈 训练输出示例

```
======================================================================
ViT 在 CIFAR-10 上的训练
======================================================================

设备: cuda
GPU: NVIDIA GeForce RTX 5070 Laptop GPU
显存: 8.05 GB

数据集信息:
  训练集: 50,000 张图像
  测试集: 10,000 张图像
  类别数: 10
  Batch size: 64

模型配置:
  Embedding 维度: 384
  Transformer 层数: 6
  参数量: 21.67M

======================================================================
开始训练
======================================================================

Epoch [1/20]
学习率: 0.000300
  Batch [100/782] Loss: 2.1234 | Acc: 25.34%
  Batch [200/782] Loss: 1.9876 | Acc: 30.12%
  ...
  Batch [782/782] Loss: 1.8543 | Acc: 35.67%

  训练: Loss=1.8543, Acc=35.67%
  测试: Loss=1.7234, Acc=38.45%
  用时: 87.32秒
  ✓ 保存最佳模型 (Acc=38.45%)

Epoch [2/20]
...
```

---

## 🔧 调整配置

### 如果显存不够
编辑 `train_cifar10.py`:
```python
batch_size = 32  # 改小 batch size
```

### 如果想训练更久
```python
num_epochs = 50  # 增加训练轮数
```

### 如果想用更大的模型
```python
embed_dim = 768   # ViT-Base 的配置
depth = 12
num_heads = 12
```

---

## 📊 加载已训练的模型

```python
import torch
from vit_model import VisionTransformer

# 创建模型
model = VisionTransformer(
    img_size=224,
    patch_size=16,
    num_classes=10,
    embed_dim=384,
    depth=6,
    num_heads=6
)

# 加载权重
checkpoint = torch.load('checkpoints/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# 查看训练信息
print(f"训练到第 {checkpoint['epoch']} 轮")
print(f"测试准确率: {checkpoint['test_acc']:.2f}%")
```

---

## 💡 训练技巧

### 1. 学习率调优
如果训练不收敛，可以尝试：
```python
learning_rate = 1e-4  # 降低学习率
```

### 2. 数据增强
如果过拟合，可以增加数据增强：
```python
transforms.RandomRotation(15)      # 随机旋转
transforms.ColorJitter(0.2)        # 颜色抖动
```

### 3. 正则化
如果过拟合，可以增加 dropout：
```python
drop_rate = 0.2  # 增加 dropout 概率
```

### 4. 早停
如果测试准确率不再提升，可以提前停止训练

---

## 🎓 下一步

训练完成后可以做：

1. **可视化 Attention Map**
   ```bash
   python3 visualize_attention.py
   ```

2. **分析错误样本**
   看看模型在哪些类别上容易混淆

3. **Fine-tuning**
   如果有预训练权重，可以 fine-tune

4. **记录到 Notion**
   把训练结果记录到你的研究笔记中

---

## 📝 常见问题

### Q: 训练很慢怎么办？
A:
- 检查是否使用了 GPU（应该显示 `cuda`）
- 减小 batch_size
- 减少 num_workers

### Q: 显存不足（OOM）？
A:
- 减小 batch_size（如 32 或 16）
- 使用更小的模型（embed_dim=256, depth=4）

### Q: 准确率一直很低？
A:
- 检查学习率是否太大或太小
- 增加训练轮数
- 检查数据归一化是否正确

### Q: 想看中间层的输出？
A:
- 可以在 `vit_model.py` 中添加 print 语句
- 或者使用 PyTorch 的 hooks

---

**祝训练顺利！🚀**

好好休息，明天手写代码的时候身体会更好！💪
