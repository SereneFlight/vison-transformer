"""
æµ‹è¯• ViT æ¨¡å‹çš„æ¯ä¸ªæ¨¡å—
å¸®åŠ©ç†è§£æ¯ä¸€æ­¥çš„ç»´åº¦å˜åŒ–

è¿è¡Œ: python test_model.py
"""

import torch
import torch.nn as nn
from vit_model import (
    PatchEmbed,
    Attention,
    MLP,
    Block,
    VisionTransformer,
    vit_base_patch16_224
)


def test_patch_embed():
    """æµ‹è¯• Patch Embedding"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: Patch Embedding")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å—
    patch_embed = PatchEmbed(img_size=224, patch_size=16, in_chans=3, embed_dim=768)

    # è¾“å…¥: batch_size=2 çš„å›¾åƒ
    x = torch.randn(2, 3, 224, 224)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}  # (B, C, H, W)")

    # å‰å‘ä¼ æ’­
    out = patch_embed(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}  # (B, num_patches, embed_dim)")

    # è§£é‡Š
    print(f"\nè¯´æ˜:")
    print(f"  - åŸå›¾: 224Ã—224 = {224*224} ä¸ªåƒç´ ")
    print(f"  - Patch å¤§å°: 16Ã—16")
    print(f"  - Patch æ•°é‡: (224/16)Ã—(224/16) = {patch_embed.num_patches}")
    print(f"  - æ¯ä¸ª patch æŠ•å½±åˆ° {out.shape[-1]} ç»´")

    assert out.shape == (2, 196, 768), "å½¢çŠ¶é”™è¯¯!"
    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


def test_attention():
    """æµ‹è¯• Multi-Head Attention"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Multi-Head Attention")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å—
    attn = Attention(dim=768, num_heads=12, qkv_bias=True)

    # è¾“å…¥: (B, N, C) = (2, 197, 768)
    # 197 = 1 CLS token + 196 patches
    x = torch.randn(2, 197, 768)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}  # (B, N, C)")

    # å‰å‘ä¼ æ’­
    out = attn(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}  # (B, N, C)")

    # è§£é‡Š
    print(f"\nè¯´æ˜:")
    print(f"  - æ³¨æ„åŠ›å¤´æ•°: {attn.num_heads}")
    print(f"  - æ¯ä¸ªå¤´çš„ç»´åº¦: {attn.head_dim} (æ€»ç»´åº¦ / å¤´æ•° = {768 // 12})")
    print(f"  - ç¼©æ”¾å› å­: {attn.scale:.4f} (1/âˆš{attn.head_dim})")
    print(f"\n  è®¡ç®—æµç¨‹:")
    print(f"    1. QKV æŠ•å½±: (2, 197, 768) â†’ (2, 197, 2304)")
    print(f"    2. æ‹†åˆ†å¤šå¤´: (2, 197, 2304) â†’ (2, 12, 197, 64)")
    print(f"    3. æ³¨æ„åŠ›çŸ©é˜µ: Q@K^T â†’ (2, 12, 197, 197)")
    print(f"    4. åŠ æƒæ±‚å’Œ: attn@V â†’ (2, 12, 197, 64)")
    print(f"    5. åˆå¹¶å¤šå¤´: (2, 12, 197, 64) â†’ (2, 197, 768)")

    assert out.shape == (2, 197, 768), "å½¢çŠ¶é”™è¯¯!"
    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


def test_mlp():
    """æµ‹è¯• MLP"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: MLP (Feed-Forward Network)")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å—
    mlp = MLP(in_features=768, hidden_features=3072, out_features=768)

    # è¾“å…¥
    x = torch.randn(2, 197, 768)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}  # (B, N, C)")

    # å‰å‘ä¼ æ’­
    out = mlp(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}  # (B, N, C)")

    # è§£é‡Š
    print(f"\nè¯´æ˜:")
    print(f"  - è¾“å…¥ç»´åº¦: {mlp.fc1.in_features}")
    print(f"  - éšè—å±‚ç»´åº¦: {mlp.fc1.out_features} (æ‰©å¤§ 4 å€)")
    print(f"  - è¾“å‡ºç»´åº¦: {mlp.fc2.out_features}")
    print(f"  - æ¿€æ´»å‡½æ•°: GELU")
    print(f"\n  è®¡ç®—æµç¨‹:")
    print(f"    1. fc1: (2, 197, 768) â†’ (2, 197, 3072)")
    print(f"    2. GELU æ¿€æ´»")
    print(f"    3. fc2: (2, 197, 3072) â†’ (2, 197, 768)")

    assert out.shape == (2, 197, 768), "å½¢çŠ¶é”™è¯¯!"
    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


def test_block():
    """æµ‹è¯• Transformer Block"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: Transformer Block")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å—
    block = Block(dim=768, num_heads=12, mlp_ratio=4, qkv_bias=True)

    # è¾“å…¥
    x = torch.randn(2, 197, 768)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}  # (B, N, C)")

    # å‰å‘ä¼ æ’­
    out = block(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}  # (B, N, C)")

    # è§£é‡Š
    print(f"\nè¯´æ˜:")
    print(f"  ä¸€ä¸ª Block åŒ…å«ä¸¤ä¸ªå­å±‚:")
    print(f"    1. Self-Attention å­å±‚:")
    print(f"       x = x + Attention(LayerNorm(x))")
    print(f"    2. MLP å­å±‚:")
    print(f"       x = x + MLP(LayerNorm(x))")
    print(f"\n  å…³é”®:")
    print(f"    - Pre-Norm: LayerNorm åœ¨å­å±‚ä¹‹å‰")
    print(f"    - Residual: æ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·®è¿æ¥ (+)")

    assert out.shape == (2, 197, 768), "å½¢çŠ¶é”™è¯¯!"
    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


def test_full_model():
    """æµ‹è¯•å®Œæ•´ ViT æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: å®Œæ•´ Vision Transformer")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å‹
    model = vit_base_patch16_224(num_classes=1000)
    model.eval()

    # è¾“å…¥
    x = torch.randn(2, 3, 224, 224)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}  # (B, C, H, W)")

    # é€æ­¥æ‰“å°ä¸­é—´ç»“æœ
    print(f"\nå®Œæ•´å‰å‘ä¼ æ’­æµç¨‹:")
    print(f"-" * 60)

    with torch.no_grad():
        B = x.shape[0]

        # æ­¥éª¤ 1
        x1 = model.patch_embed(x)
        print(f"1. Patch Embedding:     {x.shape} â†’ {x1.shape}")

        # æ­¥éª¤ 2
        cls_token = model.cls_token.expand(B, -1, -1)
        x2 = torch.cat([cls_token, x1], dim=1)
        print(f"2. åŠ  CLS token:        {x1.shape} â†’ {x2.shape}")

        # æ­¥éª¤ 3
        x3 = x2 + model.pos_embed
        print(f"3. åŠ ä½ç½®ç¼–ç :          {x2.shape} â†’ {x3.shape}")

        # æ­¥éª¤ 4
        x4 = model.pos_drop(x3)
        for i, block in enumerate(model.blocks):
            x4 = block(x4)
            if i == 0:
                print(f"4. Transformer Block 1: {x3.shape} â†’ {x4.shape}")
        print(f"   ...é€šè¿‡ 12 å±‚...")
        print(f"   Transformer Block 12: {x4.shape} â†’ {x4.shape}")

        # æ­¥éª¤ 5
        x5 = model.norm(x4)
        print(f"5. Layer Norm:          {x4.shape} â†’ {x5.shape}")

        # æ­¥éª¤ 6
        cls_output = x5[:, 0]
        print(f"6. æå– CLS token:      {x5.shape} â†’ {cls_output.shape}")

        # æ­¥éª¤ 7
        logits = model.head(cls_output)
        print(f"7. åˆ†ç±»å¤´:              {cls_output.shape} â†’ {logits.shape}")

    print(f"-" * 60)

    # å¯¹æ¯”ç›´æ¥è°ƒç”¨
    with torch.no_grad():
        out_direct = model(torch.randn(2, 3, 224, 224))

    assert logits.shape == out_direct.shape, "å½¢çŠ¶ä¸ä¸€è‡´!"
    assert logits.shape == (2, 1000), "è¾“å‡ºå½¢çŠ¶é”™è¯¯!"

    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


def test_different_variants():
    """æµ‹è¯•ä¸åŒè§„æ¨¡çš„ ViT"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: ä¸åŒè§„æ¨¡çš„ ViT å˜ä½“")
    print("=" * 60)

    from vit_model import vit_large_patch16_224, vit_huge_patch14_224

    variants = [
        ("ViT-Base/16", vit_base_patch16_224(num_classes=1000)),
        ("ViT-Large/16", vit_large_patch16_224(num_classes=1000)),
        ("ViT-Huge/14", vit_huge_patch14_224(num_classes=1000)),
    ]

    print(f"\n{'æ¨¡å‹':<15} {'å‚æ•°é‡':<15} {'Embed Dim':<12} {'Depth':<8} {'Heads':<8}")
    print(f"-" * 60)

    for name, model in variants:
        params = sum(p.numel() for p in model.parameters())
        embed_dim = model.embed_dim
        depth = len(model.blocks)
        num_heads = model.blocks[0].attn.num_heads

        print(f"{name:<15} {params/1e6:>6.1f}M        {embed_dim:<12} {depth:<8} {num_heads:<8}")

    print(f"\nâœ… æ‰€æœ‰å˜ä½“åˆ›å»ºæˆåŠŸ!")


def test_parameter_count():
    """è¯¦ç»†åˆ†æå‚æ•°é‡"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 7: å‚æ•°é‡è¯¦ç»†åˆ†æ")
    print("=" * 60)

    model = vit_base_patch16_224(num_classes=1000)

    print(f"\n{'æ¨¡å—':<30} {'å‚æ•°é‡':<15} {'å½¢çŠ¶'}")
    print(f"-" * 70)

    total = 0
    for name, param in model.named_parameters():
        params = param.numel()
        total += params
        # åªæ‰“å°å…³é”®å±‚
        if any(key in name for key in ['patch_embed', 'cls_token', 'pos_embed',
                                         'blocks.0', 'blocks.11', 'head']):
            print(f"{name:<30} {params:>12,}   {list(param.shape)}")

    print(f"-" * 70)
    print(f"{'æ€»è®¡':<30} {total:>12,}   ({total/1e6:.2f}M)")

    print(f"\nâœ… æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Vision Transformer å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 60)

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_patch_embed()
    test_attention()
    test_mlp()
    test_block()
    test_full_model()
    test_different_variants()
    test_parameter_count()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ViT å®ç°æ­£ç¡®!")
    print("=" * 60)
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. å¯è§†åŒ– Attention Map (è¿è¡Œ visualize_attention.py)")
    print("  2. åœ¨ CIFAR-10 ä¸Šè®­ç»ƒ (è¿è¡Œ train_cifar10.py)")
    print("  3. è®°å½•åˆ° Notion")
    print("=" * 60 + "\n")
