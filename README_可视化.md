# ViT å¯è§†åŒ–å®Œæ•´æŒ‡å—

## ğŸ“š å¯è§†åŒ–ç±»å‹æ€»è§ˆ

æˆ‘ä¸ºä½ å‡†å¤‡äº† **7 ç§ä¸åŒçš„å¯è§†åŒ–**ï¼Œæ¶µç›–äº† ViT è®ºæ–‡ä¸­æœ€ç»å…¸çš„å›¾åƒï¼

### ğŸ¨ å¯è§†åŒ–åˆ—è¡¨

| ç±»å‹ | æ–‡ä»¶ | è¯´æ˜ | è®ºæ–‡ä¸­å¸¸è§åº¦ |
|-----|------|------|------------|
| **1. Patch Grid** | `visualize_attention.py` | å›¾åƒå¦‚ä½•è¢«åˆ‡æˆ 16Ã—16 çš„ patches | â­â­â­ |
| **2. Attention Map** | `visualize_attention.py` | æ¨¡å‹å…³æ³¨å›¾åƒçš„å“ªäº›åŒºåŸŸï¼ˆæœ€ç»å…¸ï¼ï¼‰ | â­â­â­â­â­ |
| **3. All Attention Heads** | `visualize_attention.py` | ä¸åŒ head å­¦åˆ°çš„ä¸åŒæ¨¡å¼ | â­â­â­â­ |
| **4. Position Embedding** | `visualize_advanced.py` | ä½ç½®ç¼–ç çš„ç›¸ä¼¼åº¦ | â­â­â­ |
| **5. Attention Distance** | `visualize_advanced.py` | æµ…å±‚ vs æ·±å±‚ï¼ˆLocal vs Globalï¼‰ | â­â­â­â­ |
| **6. CLS Token Evolution** | `visualize_advanced.py` | CLS token åœ¨æ¯å±‚çš„å˜åŒ– | â­â­â­ |
| **7. Attention Rollout** | `visualize_advanced.py` | ç´¯ç§¯æ‰€æœ‰å±‚çš„ attention | â­â­â­â­ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šè¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼ˆæ¨èï¼‰

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate vla_learning

# è¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼ˆä¼šç”Ÿæˆæ‰€æœ‰ 7 ç§å¯è§†åŒ–ï¼‰
python run_visualization_demo.py
```

**è¿™ä¸ªè„šæœ¬ä¼šï¼š**
1. è‡ªåŠ¨ä» CIFAR-10 ä¸‹è½½ä¸€å¼ ç¤ºä¾‹å›¾åƒ
2. ç”Ÿæˆæ‰€æœ‰ 7 ç§å¯è§†åŒ–
3. ä¿å­˜ä¸º `vis_1_*.png` åˆ° `vis_7_*.png`

**è¿è¡Œæ—¶é—´ï¼š** çº¦ 2-3 åˆ†é’Ÿ

---

### æ–¹æ³•2ï¼šè‡ªå®šä¹‰å¯è§†åŒ–

å¦‚æœä½ æƒ³ç”¨**è‡ªå·±çš„å›¾åƒ**ï¼Œå¯ä»¥è¿™æ ·ï¼š

```python
from visualize_attention import ViTWithAttention, visualize_attention_map

# 1. åˆ›å»ºæ¨¡å‹
model = ViTWithAttention(
    img_size=224,
    patch_size=16,
    num_classes=10,
    embed_dim=768,
    depth=12,
    num_heads=12
)

# 2. å¯è§†åŒ–ä½ çš„å›¾åƒ
visualize_attention_map(
    model,
    'your_image.jpg',  # æ›¿æ¢æˆä½ çš„å›¾åƒè·¯å¾„
    save_path='my_attention.png',
    layer_indices=[0, 3, 6, 11],  # å¯è§†åŒ–å“ªå‡ å±‚
    head_index=0  # å¯è§†åŒ–ç¬¬å‡ ä¸ª head
)
```

---

## ğŸ“Š å„ç§å¯è§†åŒ–è¯¦è§£

### 1. **Patch Grid** - å›¾åƒåˆ‡åˆ†

**æ•ˆæœï¼š** æ˜¾ç¤ºå›¾åƒå¦‚ä½•è¢«åˆ‡æˆ 14Ã—14 = 196 ä¸ª patches

```python
from visualize_attention import visualize_patch_grid

visualize_patch_grid('your_image.jpg', save_path='patch_grid.png')
```

**è¯´æ˜ï¼š**
- æ¯ä¸ª patch æ˜¯ 16Ã—16 åƒç´ 
- çº¢è‰²ç½‘æ ¼æ˜¾ç¤ºåˆ‡åˆ†ä½ç½®
- æ•°å­—è¡¨ç¤º patch çš„ç´¢å¼•ï¼ˆ0-195ï¼‰

---

### 2. **Attention Map** â­ æœ€ç»å…¸ï¼

**æ•ˆæœï¼š** æ˜¾ç¤ºæ¨¡å‹"çœ‹"å›¾åƒçš„å“ªäº›éƒ¨åˆ†ï¼ˆè®ºæ–‡ä¸­æœ€å¸¸è§çš„å›¾ï¼‰

```python
from visualize_attention import ViTWithAttention, visualize_attention_map

model = ViTWithAttention(img_size=224, patch_size=16, num_classes=10,
                         embed_dim=768, depth=12, num_heads=12)

visualize_attention_map(
    model,
    'image.jpg',
    save_path='attention_map.png',
    layer_indices=[0, 2, 5, 11],  # ç¬¬0, 2, 5, 11å±‚
    head_index=0  # ç¬¬0ä¸ª attention head
)
```

**å‚æ•°è¯´æ˜ï¼š**
- `layer_indices`: é€‰æ‹©å“ªå‡ å±‚è¿›è¡Œå¯è§†åŒ–
  - æµ…å±‚ï¼ˆ0-2ï¼‰ï¼šé€šå¸¸å…³æ³¨å±€éƒ¨ç»†èŠ‚
  - æ·±å±‚ï¼ˆ9-11ï¼‰ï¼šé€šå¸¸å…³æ³¨å…¨å±€è¯­ä¹‰
- `head_index`: é€‰æ‹©å“ªä¸ª attention headï¼ˆ0-11ï¼‰

**è§£è¯»ï¼š**
- çº¢è‰²/é»„è‰²åŒºåŸŸ = é«˜ attentionï¼ˆæ¨¡å‹é‡ç‚¹å…³æ³¨ï¼‰
- è“è‰²/ç´«è‰²åŒºåŸŸ = ä½ attentionï¼ˆæ¨¡å‹è¾ƒå°‘å…³æ³¨ï¼‰

---

### 3. **All Attention Heads** - ä¸åŒå¤´çš„æ¨¡å¼

**æ•ˆæœï¼š** æ˜¾ç¤ºæŸä¸€å±‚æ‰€æœ‰ 12 ä¸ª heads çš„ attention

```python
from visualize_attention import visualize_all_heads

visualize_all_heads(
    model,
    'image.jpg',
    save_path='all_heads.png',
    layer_idx=5  # å¯è§†åŒ–ç¬¬5å±‚
)
```

**è¯´æ˜ï¼š**
- ä¸åŒ head å­¦åˆ°ä¸åŒçš„æ¨¡å¼
- æœ‰çš„ head å…³æ³¨è¾¹ç¼˜ï¼Œæœ‰çš„å…³æ³¨çº¹ç†ï¼Œæœ‰çš„å…³æ³¨æ•´ä½“

---

### 4. **Position Embedding** - ä½ç½®ç¼–ç ç›¸ä¼¼åº¦

**æ•ˆæœï¼š** æ˜¾ç¤ºå“ªäº›ä½ç½®åœ¨æ¨¡å‹çœ¼ä¸­æ˜¯"ç›¸è¿‘"çš„

```python
from visualize_advanced import visualize_position_embedding

visualize_position_embedding(model, save_path='pos_embed.png')
```

**3 ä¸ªå­å›¾ï¼š**
1. **ç›¸ä¼¼åº¦çŸ©é˜µ** - æ‰€æœ‰ patches ä¹‹é—´çš„ç›¸ä¼¼åº¦
2. **ä¸­å¿ƒ patch** - ä¸­å¿ƒ patch ä¸å…¶ä»–ä½ç½®çš„ç›¸ä¼¼åº¦
3. **PCA 2D** - é™ç»´åçš„ç©ºé—´åˆ†å¸ƒ

**è§£è¯»ï¼š**
- ç›¸é‚» patches é€šå¸¸ç›¸ä¼¼åº¦é«˜ï¼ˆé¢œè‰²äº®ï¼‰
- è·ç¦»è¿œçš„ patches ç›¸ä¼¼åº¦ä½ï¼ˆé¢œè‰²æš—ï¼‰

---

### 5. **Attention Distance** â­ Local vs Global

**æ•ˆæœï¼š** æ˜¾ç¤ºæ¯ä¸€å±‚å¹³å‡å…³æ³¨å¤šè¿œçš„ patches

```python
from visualize_advanced import visualize_attention_distance

# éœ€è¦å…ˆè¿è¡Œæ¨¡å‹è·å– attention_maps
# ... (è¿è¡Œæ¨¡å‹ä»£ç )

visualize_attention_distance(
    model.attention_maps,
    save_path='attention_distance.png'
)
```

**2 ä¸ªå­å›¾ï¼š**
1. **æ¯å±‚çš„å¹³å‡è·ç¦»** - æ›²çº¿å›¾
2. **æµ…å±‚ vs æ·±å±‚å¯¹æ¯”** - æŸ±çŠ¶å›¾

**è®ºæ–‡ä¸­çš„å‘ç°ï¼š**
- **æµ…å±‚ï¼ˆLayer 0-2ï¼‰**ï¼šå…³æ³¨é‚»è¿‘ patchesï¼ˆLocal attentionï¼‰
- **æ·±å±‚ï¼ˆLayer 10-12ï¼‰**ï¼šå…³æ³¨å…¨å±€ï¼ˆGlobal attentionï¼‰

---

### 6. **CLS Token Evolution** - CLS token çš„æ¼”å˜

**æ•ˆæœï¼š** CLS token åœ¨æ¯ä¸€å±‚çš„å˜åŒ–è½¨è¿¹

```python
from visualize_advanced import visualize_cls_token_evolution

visualize_cls_token_evolution(
    model,
    img_tensor,
    save_path='cls_evolution.png'
)
```

**2 ä¸ªå­å›¾ï¼š**
1. **PCA è½¨è¿¹** - CLS token åœ¨ç‰¹å¾ç©ºé—´çš„ç§»åŠ¨
2. **èŒƒæ•°å˜åŒ–** - CLS token å‘é‡çš„å¤§å°å˜åŒ–

**è§£è¯»ï¼š**
- ç»¿è‰²æ–¹å— = è¾“å…¥å±‚çš„ CLS token
- çº¢è‰²æ˜Ÿæ˜Ÿ = è¾“å‡ºå±‚çš„ CLS tokenï¼ˆç”¨äºåˆ†ç±»ï¼‰
- è½¨è¿¹æ˜¾ç¤º CLS token å¦‚ä½•é€æ¸"èšåˆ"ä¿¡æ¯

---

### 7. **Attention Rollout** â­ ç´¯ç§¯ Attention

**æ•ˆæœï¼š** å°†æ‰€æœ‰å±‚çš„ attention ç´¯ç§¯èµ·æ¥

```python
from visualize_advanced import visualize_attention_rollout

visualize_attention_rollout(
    model.attention_maps,
    save_path='attention_rollout.png'
)
```

**è¯´æ˜ï¼š**
- ä¸æ˜¯å•ç‹¬çœ‹æŸä¸€å±‚ï¼Œè€Œæ˜¯çœ‹"ä»è¾“å…¥åˆ°è¾“å‡º"çš„å®Œæ•´è·¯å¾„
- æ›´èƒ½åæ˜ ä¿¡æ¯çš„æµåŠ¨

**è®ºæ–‡ä¸­çš„ç”¨é€”ï¼š**
- ç†è§£æ•´ä¸ªæ¨¡å‹çš„å…³æ³¨æ¨¡å¼
- è°ƒè¯•æ¨¡å‹æ˜¯å¦å…³æ³¨æ­£ç¡®çš„åŒºåŸŸ

---

## ğŸ¯ è®ºæ–‡ä¸­æœ€å¸¸è§çš„å›¾

å¦‚æœä½ è¦å¤ç°è®ºæ–‡ä¸­çš„å›¾ï¼Œæ¨èè¿™å‡ ä¸ªï¼š

### å›¾1: Attention Mapï¼ˆå¤šå±‚å¯¹æ¯”ï¼‰

```python
visualize_attention_map(
    model, 'image.jpg',
    layer_indices=[0, 3, 6, 9, 11],
    head_index=0
)
```

### å›¾2: æ‰€æœ‰ Attention Heads

```python
visualize_all_heads(model, 'image.jpg', layer_idx=5)
```

### å›¾3: Attention Distanceï¼ˆLocal vs Globalï¼‰

```python
visualize_attention_distance(attention_maps)
```

### å›¾4: Attention Rollout

```python
visualize_attention_rollout(attention_maps)
```

---

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### æŠ€å·§1: é€‰æ‹©åˆé€‚çš„å±‚

- **æµ…å±‚ï¼ˆ0-2ï¼‰**: çœ‹å±€éƒ¨ç»†èŠ‚ï¼ˆçº¹ç†ã€è¾¹ç¼˜ï¼‰
- **ä¸­å±‚ï¼ˆ4-7ï¼‰**: çœ‹ä¸­çº§ç‰¹å¾ï¼ˆå½¢çŠ¶ã€éƒ¨ä»¶ï¼‰
- **æ·±å±‚ï¼ˆ9-11ï¼‰**: çœ‹å…¨å±€è¯­ä¹‰ï¼ˆæ•´ä½“å¯¹è±¡ï¼‰

### æŠ€å·§2: é€‰æ‹©åˆé€‚çš„ Head

ä¸åŒ head å­¦åˆ°ä¸åŒæ¨¡å¼ï¼Œå¤šè¯•å‡ ä¸ªï¼š
```python
# å¯è§†åŒ–ä¸åŒçš„ heads
for head_idx in [0, 3, 6, 9]:
    visualize_attention_map(model, 'image.jpg', head_index=head_idx)
```

### æŠ€å·§3: ä½¿ç”¨æœ‰æ„ä¹‰çš„å›¾åƒ

å»ºè®®ä½¿ç”¨ï¼š
- **å•ä¸€å¯¹è±¡** - å®¹æ˜“çœ‹å‡ºå…³æ³¨åŒºåŸŸ
- **é«˜åˆ†è¾¨ç‡** - ç»†èŠ‚æ›´æ¸…æ™°
- **æ¸…æ™°èƒŒæ™¯** - å‡å°‘å¹²æ‰°

---

## ğŸ” é«˜çº§ç”¨æ³•

### å¯¹æ¯”è®­ç»ƒå‰åçš„ Attention

```python
# è®­ç»ƒå‰
model_before = ViTWithAttention(...)
visualize_attention_map(model_before, 'image.jpg', save_path='before.png')

# è®­ç»ƒå
model_after = ViTWithAttention(...)
model_after.load_state_dict(torch.load('trained_model.pth'))
visualize_attention_map(model_after, 'image.jpg', save_path='after.png')
```

### æ‰¹é‡å¯è§†åŒ–å¤šå¼ å›¾åƒ

```python
images = ['cat.jpg', 'dog.jpg', 'bird.jpg']
for img in images:
    visualize_attention_map(model, img, save_path=f'attn_{img}')
```

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ attention map å¾ˆæ¨¡ç³Šï¼Ÿ

**ç­”ï¼š** è¿™æ˜¯æ­£å¸¸çš„ï¼å› ä¸ºï¼š
1. Attention æ˜¯åœ¨ 14Ã—14 çš„ patch ç½‘æ ¼ä¸Šè®¡ç®—çš„
2. æˆ‘ä»¬æ’å€¼åˆ° 224Ã—224 æ˜¾ç¤ºï¼Œä¼šæœ‰æ¨¡ç³Š
3. å¯ä»¥å¢åŠ  `interpolation='nearest'` æ˜¾ç¤ºå—çŠ¶æ•ˆæœ

### Q2: ä¸åŒ head çš„ attention ä¸ºä»€ä¹ˆå·®åˆ«å¾ˆå¤§ï¼Ÿ

**ç­”ï¼š** è¿™æ­£æ˜¯ Multi-Head Attention çš„ä¼˜åŠ¿ï¼
- ä¸åŒ head å­¦ä¹ ä¸åŒçš„æ¨¡å¼
- æœ‰çš„ head å…³æ³¨è¾¹ç¼˜ï¼Œæœ‰çš„å…³æ³¨çº¹ç†ï¼Œæœ‰çš„å…³æ³¨æ•´ä½“
- è¿™æ ·æ¨¡å‹å¯ä»¥ä»å¤šä¸ªè§’åº¦ç†è§£å›¾åƒ

### Q3: æµ…å±‚å’Œæ·±å±‚çš„ attention ä¸ºä»€ä¹ˆä¸åŒï¼Ÿ

**ç­”ï¼š** è¿™åæ˜ äº†ç‰¹å¾çš„å±‚æ¬¡æ€§ï¼š
- **æµ…å±‚**ï¼šå…³æ³¨ä½çº§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰â†’ Local attention
- **æ·±å±‚**ï¼šå…³æ³¨é«˜çº§è¯­ä¹‰ï¼ˆæ•´ä½“å¯¹è±¡ï¼‰â†’ Global attention

### Q4: å¦‚ä½•ç†è§£ Attention Rolloutï¼Ÿ

**ç­”ï¼š** Rollout ç´¯ç§¯äº†æ‰€æœ‰å±‚çš„ attentionï¼š
- å•å±‚ attentionï¼šåªçœ‹ä¸€å±‚çš„å…³æ³¨
- Rolloutï¼šçœ‹ä¿¡æ¯ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´è·¯å¾„
- æ›´èƒ½åæ˜ æ•´ä¸ªæ¨¡å‹çš„è¡Œä¸º

---

## ğŸ¨ å¯è§†åŒ–ç¤ºä¾‹æ•ˆæœ

è¿è¡Œ `python run_visualization_demo.py` åï¼Œä½ ä¼šå¾—åˆ°ï¼š

```
vis_1_patch_grid.png          # Patch åˆ‡åˆ†ç½‘æ ¼
vis_2_position_embedding.png  # ä½ç½®ç¼–ç ç›¸ä¼¼åº¦
vis_3_attention_layers.png    # å¤šå±‚ Attention å¯¹æ¯”
vis_4_all_heads.png           # æ‰€æœ‰ heads çš„ attention
vis_5_attention_distance.png  # Local vs Global è·ç¦»åˆ†æ
vis_6_cls_evolution.png       # CLS token æ¼”å˜è½¨è¿¹
vis_7_attention_rollout.png   # ç´¯ç§¯ Attention
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **ViT è®ºæ–‡**: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
- **Attention Rollout**: "Quantifying Attention Flow in Transformers"

---

## ğŸ†˜ é‡åˆ°é—®é¢˜ï¼Ÿ

1. ç¡®ä¿æ¿€æ´»äº†ç¯å¢ƒï¼š`conda activate vla_learning`
2. ç¡®ä¿å®‰è£…äº†æ‰€æœ‰ä¾èµ–ï¼š`pip install scipy scikit-learn seaborn`
3. æ£€æŸ¥å›¾åƒè·¯å¾„æ˜¯å¦æ­£ç¡®
4. ç¡®ä¿å›¾åƒæ˜¯ RGB æ ¼å¼ï¼ˆä¸æ˜¯ RGBA æˆ–ç°åº¦å›¾ï¼‰

---

å¥½å¥½äº«å—å¯è§†åŒ–çš„ä¹è¶£å§ï¼ğŸ‰
